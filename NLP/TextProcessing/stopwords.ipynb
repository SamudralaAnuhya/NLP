{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stopwords\n",
    "\n",
    "it will remove few words like that are not that much important to understand text liek (i , who ,am ) like this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anuhyasamudrala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''\n",
    "They say that at some point you just learn to let go, I must disagree. If it just takes one moment to let go, then you never really held on tightly enough. To a dream. To a goal. To a place. To a person. To anything. I believe you let go little by little. You let go a little, then hold back on, but with a little less force until you fully release yourself. And the tighter you hold on, the more force you let go with. The deeper you dive, the higher you'll fly. The closer you get, the further you'll pull away. The weaker you feel, the stronger you'll become. So do not be ashamed of your weaknesses. We all have them. You must learn to be kind to yourself. You must believe in yourself. You must learn to understand yourself. Never think that you are a bad person. Differentiate between your self-worth and your actions. To say that you are bad is different than saying that you made a mistake. You can't fix yourself, but you can fix a mistake.And remember, not one person on this earth is perfect. We all make mistakes. We all fall. We all have flaws. We just need to look within ourselves and treat ourselves as humans who are worthy of respect and hope. Do not give up on yourself . Get back up. Be brave. Be happy.” -Najwa Zebian\n",
    "\"I look around at us and you know what I see? Losers. I mean, like, folks who have lost stuff. And we have, man, we have, all of us. Homes, and our families, normal lives. And you think life takes more than it gives, but not today. Today it's giving us something. It is giving us a chance.\" Guardians of the Galaxy\n",
    "“There's no such thing as too far. You understand? You push everything as far as you can. You push and you push and you push until it starts pushing back. And then you push some goddamn more.”- Two for the Money\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#remove stopwords from the paragarph\n",
    "\n",
    "1. convert paragraph into sentence (sent_tokenize)\n",
    "2. in the loop\n",
    "convert sentence to words(word_tokenize)\n",
    "applt lematizer (for the above words which are not in stopwrods)---list comprehension\n",
    "3. join all the words for each sentence and update this to main \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "from nltk.stem import PorterStemmer , SnowballStemmer , WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wordlemma = WordNetLemmatizer()\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "# sentences\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    #convert sentence to words\n",
    "    words = word_tokenize(sentences[i])\n",
    "    words = [wordlemma.lemmatize(word , pos = 'v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They say point learn let go , I must disagree .',\n",
       " 'If take one moment let go , never really hold tightly enough .',\n",
       " 'To dream .',\n",
       " 'To goal .',\n",
       " 'To place .',\n",
       " 'To person .',\n",
       " 'To anything .',\n",
       " 'I believe let go little little .',\n",
       " 'You let go little , hold back , little less force fully release .',\n",
       " 'And tighter hold , force let go .',\n",
       " \"The deeper dive , higher 'll fly .\",\n",
       " \"The closer get , 'll pull away .\",\n",
       " \"The weaker feel , stronger 'll become .\",\n",
       " 'So ashamed weaknesses .',\n",
       " 'We .',\n",
       " 'You must learn kind .',\n",
       " 'You must believe .',\n",
       " 'You must learn understand .',\n",
       " 'Never think bad person .',\n",
       " 'Differentiate self-worth action .',\n",
       " 'To say bad different say make mistake .',\n",
       " \"You ca n't fix , fix mistake.And remember , one person earth perfect .\",\n",
       " 'We make mistake .',\n",
       " 'We fall .',\n",
       " 'We flaw .',\n",
       " 'We need look within treat humans worthy respect hope .',\n",
       " 'Do give .',\n",
       " 'Get back .',\n",
       " 'Be brave .',\n",
       " \"Be happy. ” -Najwa Zebian '' I look around us know I see ?\",\n",
       " 'Losers .',\n",
       " 'I mean , like , folks lose stuff .',\n",
       " 'And , man , , us .',\n",
       " 'Homes , families , normal live .',\n",
       " 'And think life take give , today .',\n",
       " \"Today 's give us something .\",\n",
       " \"It give us chance . ''\",\n",
       " \"Guardians Galaxy “ There 's thing far .\",\n",
       " 'You understand ?',\n",
       " 'You push everything far .',\n",
       " 'You push push push start push back .',\n",
       " 'And push goddamn more. ” - Two Money']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
